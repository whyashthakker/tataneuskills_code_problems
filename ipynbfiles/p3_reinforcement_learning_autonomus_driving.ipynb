{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Necessary Libraries\n",
        "- For numerical operations and array handling\n",
        "- For plotting and visualizing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AutonomousCar Class\n",
        "\n",
        "The `AutonomousCar` class implements a basic Q-learning algorithm for an autonomous car's decision-making process.\n",
        "\n",
        "- **`__init__`**: Initializes the Q-table and sets the learning rate, discount factor, and exploration rate (`epsilon`).\n",
        "- **`choose_action`**: Selects an action based on the current state using an Îµ-greedy policy (either explore randomly or exploit the best-known action).\n",
        "- **`learn`**: Updates the Q-table using the Q-learning formula to reinforce the action taken based on the received reward and the next state.\n",
        "\n",
        "This class is essential for training the car to navigate its environment by learning from rewards and penalties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AutonomousCar:\n",
        "    def __init__(self, states, actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        self.q_table = np.zeros((states, actions))\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.q_table.shape[1])  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploit\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        predict = self.q_table[state, action]\n",
        "        target = reward + self.discount_factor * np.max(self.q_table[next_state, :])\n",
        "        self.q_table[state, action] += self.learning_rate * (target - predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DrivingEnvironment Class\n",
        "\n",
        "The `DrivingEnvironment` class simulates a simplified driving environment where a car must navigate a lane while avoiding obstacles.\n",
        "\n",
        "- **`__init__`**: Initializes the environment with a lane of a given size and defines the possible actions (`left`, `stay`, `right`).\n",
        "- **`reset`**: Resets the environment by placing the car in the center and randomly positioning an obstacle.\n",
        "- **`step`**: Executes an action to move the car left, right, or stay. It then checks for collisions or successful navigation, providing a corresponding reward and indicating whether the episode is done.\n",
        "\n",
        "This class is crucial for testing and training the autonomous car in a controlled environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DrivingEnvironment:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.actions = ['left', 'stay', 'right']\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.car_position = self.size // 2\n",
        "        self.obstacle_position = np.random.randint(0, self.size)\n",
        "        return self.car_position\n",
        "\n",
        "    def step(self, action):\n",
        "        # Move car\n",
        "        if action == 0:  # left\n",
        "            self.car_position = max(0, self.car_position - 1)\n",
        "        elif action == 2:  # right\n",
        "            self.car_position = min(self.size - 1, self.car_position + 1)\n",
        "\n",
        "        # Check for collision or successful navigation\n",
        "        if self.car_position == self.obstacle_position:\n",
        "            reward = -10  # Collision\n",
        "            done = True\n",
        "        elif self.car_position == self.size // 2:\n",
        "            reward = 1  # Stayed in the middle lane\n",
        "            done = False\n",
        "        else:\n",
        "            reward = -1  # Moved away from middle lane\n",
        "            done = False\n",
        "\n",
        "        return self.car_position, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### visualize_environment Function\n",
        "\n",
        "The `visualize_environment` function provides a visual representation of the driving environment.\n",
        "\n",
        "- **`ax`**: The matplotlib axes object where the environment is drawn.\n",
        "- **`env`**: The current instance of the `DrivingEnvironment`.\n",
        "- **`car_position`**: The current position of the car within the environment.\n",
        "- **`action`** (optional): The action taken by the car (e.g., move left, stay, move right).\n",
        "- **`reward`** (optional): The reward received for the current action.\n",
        "- **`total_reward`**: The cumulative reward collected so far.\n",
        "\n",
        "The function visualizes the road, with the car (`C`) and obstacle (`X`) positions, and displays information such as the action taken, the reward earned, and the total reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def visualize_environment(ax, env, car_position, action=None, reward=None, total_reward=0):\n",
        "    ax.clear()\n",
        "    road = ['_'] * env.size\n",
        "    road[env.obstacle_position] = 'X'\n",
        "    road[car_position] = 'C'\n",
        "    \n",
        "    ax.set_xlim(-0.5, env.size - 0.5)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks(range(env.size))\n",
        "    ax.set_xticklabels(road)\n",
        "    \n",
        "    ax.axhline(y=0, color='k', linestyle='-', linewidth=2)\n",
        "    ax.plot(car_position, 0, 'bo', markersize=20, label='Car')\n",
        "    ax.plot(env.obstacle_position, 0, 'rx', markersize=20, label='Obstacle')\n",
        "    \n",
        "    if action is not None:\n",
        "        ax.set_title(f\"Action: {env.actions[action]}\", fontsize=16)\n",
        "    if reward is not None:\n",
        "        color = 'green' if reward > 0 else 'red'\n",
        "        ax.text(env.size/2, 0.5, f\"Reward: {reward}\", ha='center', va='center', fontsize=16, color=color)\n",
        "    ax.text(env.size/2, -0.5, f\"Total Reward: {total_reward}\", ha='center', va='center', fontsize=16)\n",
        "    \n",
        "    ax.legend(loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### train_and_visualize Function\n",
        "\n",
        "The `train_and_visualize` function trains an autonomous car agent in a driving environment over a specified number of episodes and visualizes the training process.\n",
        "\n",
        "- **`episodes`**: The number of training episodes (default is 10).\n",
        "\n",
        "**Process:**\n",
        "1. **Initialize Environment and Agent**: Creates an instance of `DrivingEnvironment` and `AutonomousCar`.\n",
        "2. **Training Loop**: For each episode, the environment is reset, and the agent performs actions based on its policy.\n",
        "   - **Action Selection**: Chooses an action using the agent's policy.\n",
        "   - **Environment Step**: Takes a step in the environment and receives feedback (next state, reward, and whether the episode is done).\n",
        "   - **Learning**: Updates the agent's knowledge using the Q-learning algorithm.\n",
        "   - **Visualization**: Visualizes the environment and updates the plot with the current state, action, reward, and total reward.\n",
        "3. **Recording Rewards**: Collects the total reward for each episode.\n",
        "4. **Output**: Prints the total reward for each episode and returns the trained agent along with the list of rewards.\n",
        "\n",
        "The function visualizes the environment during training, showing the car's and obstacle's positions and the rewards received.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_visualize(episodes=10):\n",
        "    env = DrivingEnvironment()\n",
        "    car = AutonomousCar(env.size, len(env.actions))\n",
        "    rewards = []\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        print(f\"Episode {episode + 1}\")\n",
        "\n",
        "        while not done:\n",
        "            action = car.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            car.learn(state, action, reward, next_state)\n",
        "            \n",
        "            step += 1\n",
        "            total_reward += reward\n",
        "            visualize_environment(ax, env, next_state, action, reward, total_reward)\n",
        "            plt.show()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1} finished. Total reward: {total_reward}\")\n",
        "\n",
        "    return car, rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Visualization\n",
        "\n",
        "1. **Train the Model**: \n",
        "   - Calls `train_and_visualize` with `episodes=5` to train the `AutonomousCar` agent in the `DrivingEnvironment` and visualize the process.\n",
        "   - Stores the trained agent and reward history.\n",
        "\n",
        "2. **Plot the Rewards**:\n",
        "   - Creates a plot showing the total reward per episode.\n",
        "   - Customizes the plot with labels, title, grid, and legend to visualize reward trends over episodes.\n",
        "\n",
        "3. **Display Final Q-table**:\n",
        "   - Prints the final Q-table of the trained `AutonomousCar` to inspect the learned Q-values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output\n",
        "\n",
        "### Final Q-table:\n",
        "\n",
        "[[ 0.          0.          0.        ]\n",
        "\n",
        " [-1.         -0.1         1.5969919 ]\n",
        "\n",
        " [-0.88585783  5.1653572  -1.9       ]\n",
        "\n",
        " [ 0.          0.          0.        ]\n",
        "\n",
        " [ 0.          0.          0.        ]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train the model with visualization\n",
        "trained_car, reward_history = train_and_visualize(episodes=5)\n",
        "\n",
        "# Plot the rewards\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(reward_history, marker='o', linestyle='-', color='b', label='Total Reward per Episode')\n",
        "plt.title('Reward History', fontsize=18)\n",
        "plt.xlabel('Episode', fontsize=14)\n",
        "plt.ylabel('Total Reward', fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFinal Q-table:\")\n",
        "print(trained_car.q_table)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
